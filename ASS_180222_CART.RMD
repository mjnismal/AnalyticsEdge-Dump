---
title: "R Notebook"
output: html_notebook
---



<!-- This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code.  -->

<!-- Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*.  -->

The data
The researchers grouped about 344,000 voters into different groups randomly - about 191,000 voters were a "control" group, and the rest were categorized into one of four "treatment" groups. These five groups correspond to five binary variables in the dataset.

"Civic Duty" (variable civicduty) group members were sent a letter that simply said "DO YOUR CIVIC DUTY - VOTE!"

"Hawthorne Effect" (variable hawthorne) group members were sent a letter that had the "Civic Duty" message plus the additional message "YOU ARE BEING STUDIED" and they were informed that their voting behavior would be examined by means of public records.

"Self" (variable self) group members received the "Civic Duty" message as well as the recent voting record of everyone in that household and a message stating that another message would be sent after the election with updated records.

"Neighbors" (variable neighbors) group members were given the same message as that for the "Self" group, except the message not only had the household voting records but also that of neighbors - maximizing social pressure.

"Control" (variable control) group members were not sent anything, and represented the typical voting situation.

Additional variables include sex (0 for male, 1 for female), yob (year of birth), and the dependent variable voting (1 if they voted, 0 otherwise).


We will first get familiar with the data. Load the CSV file gerber.csv into R. What proportion of people in this dataset voted in this election?
```{r}
gerber = read.csv("https://prod-edxapp.edx-cdn.org/assets/courseware/v1/56161c22ffa80cfc8637334891d6d402/asset-v1:MITx+15.071x+2T2017+type@asset+block/gerber.csv")
head(gerber)
table(gerber$voting)/nrow(gerber)
```

Which of the four "treatment groups" had the largest percentage of people who actually voted (voting = 1)?
```{r}
table(gerber$civicduty,gerber$voting)
table(gerber$hawthorne,gerber$voting)
table(gerber$self,gerber$voting)
table(gerber$neighbors,gerber$voting)
```

<!-- Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*. -->

<!-- When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file). -->


Build a logistic regression model for voting using the four treatment group variables as the independent variables (civicduty, hawthorne, self, and neighbors). Use all the data to build the model (DO NOT split the data into a training set and testing set). Which of the following coefficients are significant in the logistic regression model? Select all that apply.

```{r}
gerber_log= glm(voting~civicduty+hawthorne+self+neighbors,data=gerber,family = "binomial")
summary(gerber_log)

```


Using a threshold of 0.3, what is the accuracy of the logistic regression model? (When making predictions, you don't need to use the newdata argument since we didn't split our data.)
```{r}
pred = predict(gerber_log,type ="response")
table(gerber$voting,pred>0.3)
(134513+51966)/nrow(gerber)
```

Using a threshold of 0.5, what is the accuracy of the logistic regression model?
```{r}
table(gerber$voting,pred>0.5)
(235388)/nrow(gerber)

```

Compare your previous two answers to the percentage of people who did not vote (the baseline accuracy) and compute the AUC of the model. What is happening here?
```{r}
# Run once per instance
# install.packages("ROCR")
library(ROCR)
ROCRpred = prediction(pred, gerber$voting)
as.numeric(performance(ROCRpred, "auc")@y.values)
```

We will now try out trees. Build a CART tree for voting using all data and the same four treatment variables we used before. Don't set the option method="class" - we are actually going to create a regression tree here. We are interested in building a tree to explore the fraction of people who vote, or the probability of voting. We'd like CART to split our groups if they have different probabilities of voting. If we used method='class', CART would only split if one of the groups had a probability of voting above 50% and the other had a probability of voting less than 50% (since the predicted outcomes would be different). However, with regression trees, CART will split even if both groups have probability less than 50%.

```{r}
library(rpart)
library(rpart.plot)
CARTmodel = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber)
prp(CARTmodel)
```

Now build the tree using the command:

CARTmodel2 = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, cp=0.0)

to force the complete tree to be built. Then plot the tree. What do you observe about the order of the splits?
```{r}
CARTmodel2 = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, cp=0.0)
prp(CARTmodel2)
```


Make a new tree that includes the "sex" variable, again with cp = 0.0. Notice that sex appears as a split that is of secondary importance to the treatment group.

In the control group, which gender is more likely to vote?
```{r}

CARTmodel2 = rpart(voting ~ civicduty + hawthorne + self + neighbors+sex, data=gerber, cp=0.0)
prp(CARTmodel2)
```

We know trees can handle "nonlinear" relationships, e.g. "in the 'Civic Duty' group and female", but as we will see in the next few questions, it is possible to do the same for logistic regression. First, let's explore what trees can tell us some more.

Let's just focus on the "Control" treatment group. Create a regression tree using just the "control" variable, then create another tree with the "control" and "sex" variables, both with cp=0.0.

In the "control" only tree, what is the absolute value of the difference in the predicted probability of voting between being in the control group versus being in a different group? You can use the absolute value function to get answer, i.e. abs(Control Prediction - Non-Control Prediction). Add the argument "digits = 6" to the prp command to get a more accurate estimate.
```{r}
CARTmodel3 = rpart(voting ~ control, data=gerber, cp=0.0)
prp(CARTmodel3,digits=6)
.34-.296638
```

Now, using the second tree (with control and sex), determine who is affected more by NOT being in the control group (being in any of the four treatment groups):
```{r}
CARTmodel4 = rpart(voting ~ control+sex, data=gerber, cp=0.0)
prp(CARTmodel4,digits=6)
.334176-.290456
.345818-.302795
```

Going back to logistic regression now, create a model using "sex" and "control". Interpret the coefficient for "sex":
```{r}
gerber_log2= glm(voting~sex+control,data=gerber,family = "binomial")
summary(gerber_log2)
```

The regression tree calculated the percentage voting exactly for every one of the four possibilities (Man, Not Control), (Man, Control), (Woman, Not Control), (Woman, Control). Logistic regression has attempted to do the same, although it wasn't able to do as well because it can't consider exactly the joint possibility of being a women and in the control group.

We can quantify this precisely. Create the following dataframe (this contains all of the possible values of sex and control), and evaluate your logistic regression using the predict function (where "LogModelSex" is the name of your logistic regression model that uses both control and sex):

The four values in the results correspond to the four possibilities in the order they are stated above ( (Man, Not Control), (Man, Control), (Woman, Not Control), (Woman, Control) ). What is the absolute difference between the tree and the logistic regression for the (Woman, Control) case? Give an answer with five numbers after the decimal point.
```{r}
Possibilities = data.frame(sex=c(0,0,1,1),control=c(0,1,0,1))
predict(gerber_log2, newdata=Possibilities, type="response")
.290456-.2908065
```

So the difference is not too big for this dataset, but it is there. We're going to add a new term to our logistic regression now, that is the combination of the "sex" and "control" variables - so if this new variable is 1, that means the person is a woman AND in the control group. We can do that with the following command:

LogModel2 = glm(voting ~ sex + control + sex:control, data=gerber, family="binomial")

How do you interpret the coefficient for the new variable in isolation? That is, how does it relate to the dependent variable?
```{r}
gerber_log2= glm(voting~sex+control+sex:control,data=gerber,family = "binomial")
summary(gerber_log2)
```

Run the same code as before to calculate the average for each group:

predict(LogModel2, newdata=Possibilities, type="response")

Now what is the difference between the logistic regression model and the CART model for the (Woman, Control) case? Again, give your answer with five numbers after the decimal point.
```{r}
predict(gerber_log2, newdata=Possibilities, type="response")
.290456-0.2904558
```


This dataset contains the following 17 variables:

letter = the letter that the image corresponds to (A, B, P or R)
xbox = the horizontal position of where the smallest box covering the letter shape begins.
ybox = the vertical position of where the smallest box covering the letter shape begins.
width = the width of this smallest box.
height = the height of this smallest box.
onpix = the total number of "on" pixels in the character image
xbar = the mean horizontal position of all of the "on" pixels
ybar = the mean vertical position of all of the "on" pixels
x2bar = the mean squared horizontal position of all of the "on" pixels in the image
y2bar = the mean squared vertical position of all of the "on" pixels in the image
xybar = the mean of the product of the horizontal and vertical position of all of the "on" pixels in the image
x2ybar = the mean of the product of the squared horizontal position and the vertical position of all of the "on" pixels
xy2bar = the mean of the product of the horizontal position and the squared vertical position of all of the "on" pixels
xedge = the mean number of edges (the number of times an "off" pixel is followed by an "on" pixel, or the image boundary is hit) as the image is scanned from left to right, along the whole vertical length of the image
xedgeycor = the mean of the product of the number of horizontal edges at each vertical position and the vertical position
yedge = the mean number of edges as the images is scanned from top to bottom, along the whole horizontal length of the image
yedgexcor = the mean of the product of the number of vertical edges at each horizontal position and the horizontal position



Let's warm up by attempting to predict just whether a letter is B or not. To begin, load the file letters_ABPR.csv into R, and call it letters. Then, create a new variable isB in the dataframe, which takes the value "TRUE" if the observation corresponds to the letter B, and "FALSE" if it does not. You can do this by typing the following command into your R console:

letters$isB = as.factor(letters$letter == "B")

Now split the data set into a training and testing set, putting 50% of the data in the training set. Set the seed to 1000 before making the split. The first argument to sample.split should be the dependent variable "letters$isB". Remember that TRUE values from sample.split should go in the training set.

Before building models, let's consider a baseline method that always predicts the most frequent outcome, which is "not B". What is the accuracy of this baseline method on the test set?
```{r}
letters = read.csv("https://prod-edxapp.edx-cdn.org/assets/courseware/v1/0a0ec2047c07bcb6063f39b8b622288b/asset-v1:MITx+15.071x+2T2017+type@asset+block/letters_ABPR.csv")
letters$isB = as.factor(letters$letter == "B")

library(caTools)
set.seed(1000)
isB2 = sample.split(letters$isB,SplitRatio = 0.5)
letters_trn = subset(letters,isB2==T)
letters_tst = subset(letters,isB2==F)
head(letters_trn)
head(letters_tst)

table(letters_tst$isB)/nrow(letters_tst)
```


Now build a classification tree to predict whether a letter is a B or not, using the training set to build your model. Remember to remove the variable "letter" out of the model, as this is related to what we are trying to predict! To just remove one variable, you can either write out the other variables, or remember what we did in the Billboards problem in Week 3, and use the following notation:

CARTb = rpart(isB ~ . - letter, data=train, method="class")

We are just using the default parameters in our CART model, so we don't need to add the minbucket or cp arguments at all. We also added the argument method="class" since this is a classification problem.

What is the accuracy of the CART model on the test set? (Use type="class" when making predictions on the test set.)

```{r}
library(rpart)
library(rpart.plot)
CARTb = rpart(isB ~ . - letter, data=letters_trn, method="class")
prp(CARTb)
CARTb_pred = predict(CARTb,newdata = letters_tst,type = "class")
table(letters_tst$isB,CARTb_pred)
(1118+340)/nrow(letters_tst)
```

Now, build a random forest model to predict whether the letter is a B or not (the isB variable) using the training set. You should use all of the other variables as independent variables, except letter (since it helped us define what we are trying to predict!). Use the default settings for ntree and nodesize (don't include these arguments at all). Right before building the model, set the seed to 1000. (NOTE: You might get a slightly different answer on this problem, even if you set the random seed. This has to do with your operating system and the implementation of the random forest algorithm.)

What is the accuracy of the model on the test set?
```{r}
# install.packages("randomForest")
library(randomForest)
set.seed(1000)
isB_rf = randomForest(isB ~ . - letter,data=letters_trn)
predict_rf= predict(isB_rf,newdata = letters_tst)
table(letters_tst$isB,predict_rf)
(1165+374)/nrow(letters_tst)
```

Let us now move on to the problem that we were originally interested in, which is to predict whether or not a letter is one of the four letters A, B, P or R.

As we saw in the D2Hawkeye lecture, building a multiclass classification CART model in R is no harder than building the models for binary classification problems. Fortunately, building a random forest model is just as easy.

The variable in our data frame which we will be trying to predict is "letter". Start by converting letter in the original data set (letters) to a factor by running the following command in R:

letters$letter = as.factor( letters$letter )

Now, generate new training and testing sets of the letters data frame using letters$letter as the first input to the sample.split function. Before splitting, set your seed to 2000. Again put 50% of the data in the training set. (Why do we need to split the data again? Remember that sample.split balances the outcome variable in the training and testing sets. With a new outcome variable, we want to re-generate our split.)

In a multiclass classification problem, a simple baseline model is to predict the most frequent class of all of the options.

What is the baseline accuracy on the testing set?

```{r}
letters$letter = as.factor(letters$letter)
set.seed(2000)
letters$ltrs = sample.split(letters$letter,SplitRatio = .5)
letters_trn = subset(letters,ltrs==T)
letters_tst = subset(letters,ltrs==F)
head(letters_trn)
head(letters_tst)
table(letters_tst$letter)/nrow(letters_tst)

```

Now build a classification tree to predict "letter", using the training set to build your model. You should use all of the other variables as independent variables, except "isB", since it is related to what we are trying to predict! Just use the default parameters in your CART model. Add the argument method="class" since this is a classification problem. Even though we have multiple classes here, nothing changes in how we build the model from the binary case.

What is the test set accuracy of your CART model? Use the argument type="class" when making predictions.

(HINT: When you are computing the test set accuracy using the confusion matrix, you want to add everything on the main diagonal and divide by the total number of observations in the test set, which can be computed with nrow(test), where test is the name of your test set).

```{r}

CARTb_ltr = rpart(letter ~ . - isB, data=letters_trn, method="class")
ltr_pred = predict(CARTb_ltr,newdata = letters_tst,type="class")
table(letters_tst$letter,ltr_pred)
(348+318+363+340)/nrow(letters_tst)
```

Now build a random forest model on the training data, using the same independent variables as in the previous problem -- again, don't forget to remove the isB variable. Just use the default parameter values for ntree and nodesize (you don't need to include these arguments at all). Set the seed to 1000 right before building your model. (Remember that you might get a slightly different result even if you set the random seed.)

What is the test set accuracy of your random forest model?

```{r}
set.seed(1000)
ltr_rf = randomForest(letter ~ . -isB,data=letters_trn)
ltr_predrf= predict(ltr_rf,newdata = letters_tst)
table(letters_tst$letter,ltr_predrf)
(390+380+395+367)/nrow(letters_tst)
```
#### _Might differ from the answer due to changes in algorithm over the years_



The dataset includes the following 13 variables:

age = the age of the individual in years
workclass = the classification of the individual's working status (does the person work for the federal government, work for the local government, work without pay, and so on)
education = the level of education of the individual (e.g., 5th-6th grade, high school graduate, PhD, so on)
maritalstatus = the marital status of the individual
occupation = the type of work the individual does (e.g., administrative/clerical work, farming/fishing, sales and so on)
relationship = relationship of individual to his/her household
race = the individual's race
sex = the individual's sex
capitalgain = the capital gains of the individual in 1994 (from selling an asset such as a stock or bond for more than the original purchase price)
capitalloss = the capital losses of the individual in 1994 (from selling an asset such as a stock or bond for less than the original purchase price)
hoursperweek = the number of hours the individual works per week
nativecountry = the native country of the individual
over50k = whether or not the individual earned more than $50,000 in 1994

Let's begin by building a logistic regression model to predict whether an individual's earnings are above $50,000 (the variable "over50k") using all of the other variables as independent variables. First, read the dataset census.csv into R.

Then, split the data randomly into a training set and a testing set, setting the seed to 2000 before creating the split. Split the data so that the training set contains 60% of the observations, while the testing set contains 40% of the observations.

Next, build a logistic regression model to predict the dependent variable "over50k", using all of the other variables in the dataset as independent variables. Use the training set to build the model.

Which variables are significant, or have factors that are significant? (Use 0.1 as your significance threshold, so variables with a period or dot in the stars column should be counted too. You might see a warning message here - you can ignore it and proceed. This message is a warning that we might be overfitting our model to the training set.) Select all that apply.

```{r}
census1994 = read.csv("https://prod-edxapp.edx-cdn.org/assets/courseware/v1/c64bd5b81803ec9b31803810db517893/asset-v1:MITx+15.071x+2T2017+type@asset+block/census.csv")
set.seed(2000)
census1994$split= sample.split(census1994$over50k,SplitRatio = .6)
census1994_TR = subset(census1994,split == T)
census1994_TS = subset(census1994,split == F)
censuslogmdl = glm(over50k~.,data = census1994_TR,family = "binomial")
summary(censuslogmdl)
```

What is the accuracy of the model on the testing set? Use a threshold of 0.5. (You might see a warning message when you make predictions on the test set - you can safely ignore it.)

```{r}
test_pr = predict(censuslogmdl,newdata= census1994_TS,type ="response")
table(test_pr>.5,census1994_TS$over50k)
(9051+1888)/nrow(census1994_TS)
```

What is the baseline accuracy for the testing set?
```{r}
table(census1994_TS$over50k)/nrow(census1994_TS)
```

What is the area-under-the-curve (AUC) for this model on the test set?
```{r}
ROCRpred <- prediction(test_pr, census1994_TS$over50k)
as.numeric(performance(ROCRpred, "auc")@y.values)
```

We have just seen how the logistic regression model for this data achieves a high accuracy. Moreover, the significances of the variables give us a way to gauge which variables are relevant for this prediction task. However, it is not immediately clear which variables are more important than the others, especially due to the large number of factor variables in this problem.

Let us now build a classification tree to predict "over50k". Use the training set to build the model, and all of the other variables as independent variables. Use the default parameters, so don't set a value for minbucket or cp. Remember to specify method="class" as an argument to rpart, since this is a classification problem. After you are done building the model, plot the resulting tree.

How many splits does the tree have in total?
```{r}
census_DT = rpart(over50k ~ ., data=census1994_TR)
prp(census_DT)
```

What is the accuracy of the model on the testing set? Use a threshold of 0.5. (You can either add the argument type="class", or generate probabilities and use a threshold of 0.5 like in logistic regression.)  

```{r}
census_DT_pred = predict(census_DT,newdata=census1994_TS,type="class")
table(census_DT_pred,census1994_TS$over50k)
(9243+1596)/nrow(census1994_TS)
```


Let us now consider the ROC curve and AUC for the CART model on the test set. You will need to get predicted probabilities for the observations in the test set to build the ROC curve and compute the AUC. Remember that you can do this by removing the type="class" argument when making predictions, and taking the second column of the resulting object.

Plot the ROC curve for the CART model you have estimated. Observe that compared to the logistic regression ROC curve, the CART ROC curve is less smooth than the logistic regression ROC curve. Which of the following explanations for this behavior is most correct? (HINT: Think about what the ROC curve is plotting and what changing the threshold does.)
```{r}
census_DT_pred2= predict(census_DT,newdata=census1994_TS)
ROCRpred <- prediction(census_DT_pred2[,2], census1994_TS$over50k)
ROCRperf <- performance(ROCRpred, 'tpr','fpr')
plot(ROCRperf, colorize = TRUE, text.adj = c(-0.2,1.7))
```

What is the AUC of the CART model on the test set?

```{r}
as.numeric(performance(ROCRpred, "auc")@y.values)
```

Before building a random forest model, we'll down-sample our training set. While some modern personal computers can build a random forest model on the entire training set, others might run out of memory when trying to train the model since random forests is much more computationally intensive than CART or Logistic Regression. For this reason, before continuing we will define a new training set to be used when building our random forest model, that contains 2000 randomly selected obervations from the original training set. Do this by running the following commands in your R console (assuming your training set is called "train"):

set.seed(1)

trainSmall = train[sample(nrow(train), 2000), ]

Let us now build a random forest model to predict "over50k", using the dataset "trainSmall" as the data used to build the model. Set the seed to 1 again right before building the model, and use all of the other variables in the dataset as independent variables. (If you get an error that random forest "can not handle categorical predictors with more than 32 categories", re-build the model without the nativecountry variable as one of the independent variables.)

Then, make predictions using this model on the entire test set. What is the accuracy of the model on the test set, using a threshold of 0.5? (Remember that you don't need a "type" argument when making predictions with a random forest model if you want to use a threshold of 0.5. Also, note that your accuracy might be different from the one reported here, since random forest models can still differ depending on your operating system, even when the random seed is set. )

```{r}
set.seed(1)
trainSmall = census1994_TR[sample(nrow(census1994_TR), 2000), ]
census_rf = randomForest(over50k~.,data = trainSmall)
census_rf_pred = predict(census_rf,newdata = census1994_TS)
table(census_rf_pred,census1994_TS$over50k)
(9568+1189)/nrow(census1994_TS)
```



As we discussed in lecture, random forest models work by building a large collection of trees. As a result, we lose some of the interpretability that comes with CART in terms of seeing how predictions are made and which variables are important. However, we can still compute metrics that give us insight into which variables are important.

One metric that we can look at is the number of times, aggregated over all of the trees in the random forest model, that a certain variable is selected for a split. To view this metric, run the following lines of R code (replace "MODEL" with the name of your random forest model):

vu = varUsed(MODEL, count=TRUE)

vusorted = sort(vu, decreasing = FALSE, index.return = TRUE)

dotchart(vusorted$x, names(MODEL$forest$xlevels[vusorted$ix]))

This code produces a chart that for each variable measures the number of times that variable was selected for splitting (the value on the x-axis). Which of the following variables is the most important in terms of the number of splits?

```{r}
vu = varUsed(census_rf, count=TRUE)
vusorted = sort(vu, decreasing = FALSE, index.return = TRUE)
dotchart(vusorted$x, names(census_rf$forest$xlevels[vusorted$ix]))
```

A different metric we can look at is related to "impurity", which measures how homogenous each bucket or leaf of the tree is. In each tree in the forest, whenever we select a variable and perform a split, the impurity is decreased. Therefore, one way to measure the importance of a variable is to average the reduction in impurity, taken over all the times that variable is selected for splitting in all of the trees in the forest. To compute this metric, run the following command in R (replace "MODEL" with the name of your random forest model):

varImpPlot(MODEL)

Which one of the following variables is the most important in terms of mean reduction in impurity?

```{r}
varImpPlot(census_rf)
```

We now conclude our study of this data set by looking at how CART behaves with different choices of its parameters.

Let us select the cp parameter for our CART model using k-fold cross validation, with k = 10 folds. Do this by using the train function. Set the seed beforehand to 2. Test cp values from 0.002 to 0.1 in 0.002 increments, by using the following command:

cartGrid = expand.grid( .cp = seq(0.002,0.1,0.002))

Also, remember to use the entire training set "train" when building this model. The train function might take some time to run.

Which value of cp does the train function recommend?

```{r}
library(caret)
library(e1071)
numfolds = trainControl(method='cv',number=10)
cartGrid = expand.grid( .cp = seq(0.002,0.1,0.002))
train(over50k~.,
      data = census1994_TR,
      method = "rpart",
      trControl=numfolds,
      tuneGrid=cartGrid)

```

Fit a CART model to the training data using this value of cp. What is the prediction accuracy on the test set?

```{r}
census_cart = rpart( over50k~.,data=census1994_TR,method="class",cp=0.002)
census_cart_pred = predict(census_cart, newdata = census1994_TS,type="class")
table(census_cart_pred,census1994_TS$over50k)
(9178+1838)/nrow(census1994_TS)
prp(census_cart)
```

